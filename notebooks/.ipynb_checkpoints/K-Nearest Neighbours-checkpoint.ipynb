{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbours\n",
    "\n",
    "## Pros\n",
    "\n",
    "- Easy to implement\n",
    "- Can perform well even with limited data\n",
    "- Can be used for classification, regression, or as a meta-learner\n",
    "- Lazy learner, so easy to update\n",
    "\n",
    "## Cons\n",
    "\n",
    "- Simplistic (linear boundaries)\n",
    "- Computing distances can be expensive\n",
    "- Appropriate similarity measure has to be picked\n",
    "- Predictions can be slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the model\n",
    "\n",
    "K-Nearest Neighbour is one of the most ancient machine learning models. Formalized in 1951 by Fix and Hodges, it can be used for classification and regression and is very easy to implement. Though being simplistic, it has shown to give decent performance, especially in cases where there is not much data to work with.\n",
    "\n",
    "In other cases, it has been used as an easy-to-compute baseline, or even as a meta-learner.\n",
    "\n",
    "## Introduction \n",
    "\n",
    "K-Nearest Neighbour is based on a very common assumption in machine learning: data points that are similar/close to each other also behave similarly.\n",
    "\n",
    "In the real world, you would for instance guess that two houses that are very similar to each other could be sold for approximately the same price. Or for instance, two e-mails that are similar to each other are pretty much as likely to be spam or not.\n",
    "\n",
    "But, how do we represent this concept in our code when everything is represented by numbers? We would like to represent similarity as a real number from 0 to 1, 1 being identical, and 0 being perfectly dissimilar. Instead of trying to determine how similar two examples are, we could also calculate how far away they are from each other. Distance and similarities are two sides of the same coins as they represent the same piece of information.\n",
    "\n",
    "Thankfully, in mathematics, there already exists plenty of distance measures, many of which are referred to in our notebook about distances.\n",
    "\n",
    ">Distance and similarity are concepts that are deeply at the root of many machine learning models. To give you more intuition about it, there is also a blog post with some more intuition about this.\n",
    "\n",
    "\n",
    "## Practical Example\n",
    "\n",
    "Let's plot some data to build some more intuition about how KNN works. Here we will plot data points belonging to two classes, red (0) and green (1), and a gray point for which the class needs to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"D:/source/skratch/source\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "X = np.array([[0, 1], [1, 2], [0, 2], [1, 1.1], [1, 0.5], [0, 0.6], [0.2, 1.1], [1.1, 1.4],\n",
    "              [5, 3], [4, 4], [3.7, 5], [3.1, 4], [4, 5.5], [5.1, 4.6], [3.8, 4.1], [4, 5.4],\n",
    "              [0.5, 0.5]])\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2])\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.scatter(X[:, 0], X[:, 1],\n",
    "            c=y,\n",
    "            cmap=ListedColormap(['red', 'green', 'gray']),\n",
    "            s=100, marker='X')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, just by looking at the plot, it seems quite obvious that the gray point is surrounded by red points, and therefore, it makes sense to classify it as red. This reasoning can be essentially summarized as \"a point will have the class that most of its neighbours also have\". Similarly, if all of these points were assigned some sort of value, it would make sense to assign the gray point, a value that is similar to the value of its neighbours.\n",
    "\n",
    "This simple intuition is what K-Nearest Neighbours uses for both classification and regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation from scratch\n",
    "\n",
    "Now, let's actually build a KNN classifier and a KNN regressor. As we've shown, both share much of the same logic, and therefore, most of the code will be identical.\n",
    "\n",
    "First, we will define some generic `KNN` object. In the constructor, we pass two parameters:\n",
    "- The number of neighbours\n",
    "- Whether or not we want to use weighted distances\n",
    "\n",
    "The `fit` function is extremely simple here. Indeed, all that is needed is to store the data that inside the object. Similarly, the `update` function only concatenates the existing data with the new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementation of KNN\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils.distances import euclidean, cdist\n",
    "\n",
    "\n",
    "class KNN:\n",
    "\n",
    "    def __init__(self, k=1, weighted=False):\n",
    "\n",
    "        self.k = k\n",
    "        self.weighted = weighted\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "\n",
    "        return self\n",
    "\n",
    "    def update(self, X, y):\n",
    "\n",
    "        self.X_ = np.concatenate((self.X_, X))\n",
    "        self.y_ = np.concatenate((self.y_, y))\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to predict, we need to do two things:\n",
    "\n",
    "- Find the K-nearest neighbours by computing their distances to each of our example   \n",
    "- Given these neighbours and their distances, vote to define the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict(self, X):\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for x in X:\n",
    "\n",
    "            neighbours, distances = self._get_neighbours(x)\n",
    "\n",
    "            prediction = self._vote(neighbours, distances)\n",
    "\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving the neighbours can be done by simply calculating all pairwise distances between our example and the data stored inside the model. Once these distances are known, the K instances that have the shortest distance to the example are returned.\n",
    "\n",
    "In this case, the distance is defined as the Euclidean distance between two instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _get_neighbours(self, x):\n",
    "\n",
    "        distances = np.array([self._distance(x, x_) for x_ in self.X_])\n",
    "        indices = np.argsort(distances)[:self.k]\n",
    "\n",
    "        return self.y_[indices], distances[indices]\n",
    "\n",
    "    def _distance(self, a, b):\n",
    "\n",
    "        return euclidean(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to compute the weights that we would like to assign to each neighbour. Typically, this is done by favouring neighbours that are closer by given them a weight equal to 1 divided by their distance.\n",
    "\n",
    ">Here we also cover the situation where a neighbour might have a distance of 0. Obviously, we can't divide by zero, so we assign this neighbour a weight of 1 and all other neighbours a weight of zero. This is also how scikit-learn deals with this situation according to their source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _get_weights(self, distances):\n",
    "\n",
    "        weights = np.ones_like(distances, dtype=float)\n",
    "\n",
    "        if self.weighted:\n",
    "            if any(distances == 0):\n",
    "                weights[distances != 0] = 0\n",
    "            else:\n",
    "                weights /= distances\n",
    "\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the last thing that we need to define is how to vote to define the output. This part is the only thing that the KNN classifier and regressor do differently, and therefore, we will reserve it to their respective classes.\n",
    "\n",
    ">We added the function here to reinforce the fact that a voting scheme is needed. Don't worry about the error being thrown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _vote(self, targets, distances):\n",
    "        raise NotImplementedError(\"KNN requires a _vote function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the classification case, given the neighbours and their weights, we define the class of our example by using a weighted majority voting. It works the same way as a regular majority voting except that instead of each voter having 1 vote each, their weight represents \"how many\" votes they get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_Classifier(KNN):\n",
    "\n",
    "    def _vote(self, classes, distances):\n",
    "\n",
    "        weights = self._get_weights(distances)\n",
    "\n",
    "        frequencies = {c: np.sum(weights[classes == c]) for c in list(set(classes))}\n",
    "\n",
    "        return max(classes, key=frequencies.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the regression case, we use a simple weighted average.\n",
    "\n",
    ">https://en.wikipedia.org/wiki/Weighted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_Regressor(KNN):\n",
    "\n",
    "    def _vote(self, targets, distances):\n",
    "\n",
    "        weights = self._get_weights(distances)\n",
    "\n",
    "        return np.sum(weights * targets) / np.sum(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we could for instance use the `KNN_Classifier` to classify that gray point that we had mentioned at the beginning of the notebook.\n",
    "\n",
    "from supervised.knn import KNN_Classifier#, KNN, KNN_Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supervised.knn import KNN_Classifier#, KNN, KNN_Regressor\n",
    "\n",
    "X = np.array([[0, 1], [1, 2], [0, 2], [1, 1.1], [1, 0.5], [0, 0.6], [0.2, 1.1], [1.1, 1.4],\n",
    "              [5, 3], [4, 4], [3.7, 5], [3.1, 4], [4, 5.5], [5.1, 4.6], [3.8, 4.1], [4, 5.4]])\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "clf = KNN_Classifier(k=3)\n",
    "clf.fit(X, y)\n",
    "clf.predict([[0.5,0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the class is `0`, which corresponds to the red class.\n",
    "\n",
    "## FAQ\n",
    "\n",
    "### Why is it called a lazy learner?\n",
    "\n",
    "A lazy learner is a model that only learns when it is asked to make a prediction. KNN is a lazy learner because as we've shown, everything happens in the `predict` method as opposed to the `fit` method.\n",
    ">https://en.wikipedia.org/wiki/Lazy_learning\n",
    "\n",
    "### What is the best value for K?\n",
    "\n",
    "$K$ is a hyperparameter and picking the right one is highly dependent on the task at hand. Typically, only odd numbers are chosen to avoid ties, and because of Occam's razor, it is often assumed that smaller values of $K$ are preferable. Sometimes, a rule of thumb of chosing $K = \\frac{\\sqrt{n}}{2}$ is proposed if there are $n$ data points, but there is no strong theory behind it. \n",
    "\n",
    "### What distance measure should I use?\n",
    "\n",
    "Again, picking an appropriate distance measure can make a huge difference with KNN. Since it is a lazy learner, and therefore doesn't require training time, it is often possible to plainly compare different distance measures and pick the best one.\n",
    "\n",
    "### Why weight the neighbours?\n",
    "\n",
    "Weighting the neighbours can turn out to be useful to increase the performance, but it is always wise to experiment with and without weighting to see what performs best. In our case, we gave a higher weights to the closest neighbours, but obviously, more complex weighting schemes can be experimented.\n",
    "\n",
    "### Why is KNN a meta-learner?\n",
    "\n",
    "KNN is a meta-learner in the sense that the voting scheme can be made as complex as you want. If you'd like to replace the majority voting scheme explained in this tutorial, by a deep neural network, it is entirely possible. It is considered a meta-learner because it learns \"how to learn\" by selecting the instances on which a decision has to then be made.\n",
    "\n",
    "### Are there other ways to find the closest neighbours?\n",
    "\n",
    "The K-nearest neighbours according to a certain distance measure will always be the same, but there exists many different algorithm to find them efficiently. A famous one involves storing the data in a KD-tree. In this tutorial, we used brute force, but in some cases, this might take too much time.\n",
    ">https://en.wikipedia.org/wiki/Kd-tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful resources\n",
    "\n",
    "Sklearn:\n",
    "http://scikit-learn.org/stable/modules/neighbors.html\n",
    "\n",
    "Machine Learning Mastery:\n",
    "https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/\n",
    "\n",
    "ML from scratch:\n",
    "https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/supervised_learning/k_nearest_neighbors.py\n",
    "\n",
    "Videos:\n",
    "https://www.youtube.com/watch?v=UqYde-LULfs (Thales Sehn Körting)\n",
    "https://www.youtube.com/watch?v=4ObVzTuFivY (Mathematical Monk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
