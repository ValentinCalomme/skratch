{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbours\n",
    "\n",
    "Let’s build a K-Nearest Neighbours model from scratch.\n",
    "\n",
    "First, we will define some generic `KNN` object. In the constructor, we pass three parameters:\n",
    "\n",
    "- The number of neighbours being used to make predictions\n",
    "- The distance measure we want to use\n",
    "- Whether or not we want to use weighted distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"D:/source/skratch/source\")\n",
    "from collections import Counter\n",
    " \n",
    "import numpy as np\n",
    " \n",
    "from utils.distances import euclidean\n",
    " \n",
    " \n",
    "class KNN:\n",
    " \n",
    "    def __init__(self, k, distance=euclidean, weighted=False):\n",
    " \n",
    "        self.k = k\n",
    "        self.weighted = weighted  # Whether or not to use weighted distances\n",
    "        self.distance = distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the fit function, which is the function which describes how to train a model. For a K-Nearest Neighbours model, the training is rather simplistic. Indeed, all there needs to be done is to store the training instances as the model’s parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def fit(self, X, y):\n",
    "\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can build an update function which will update the state of the model as more data points are provided for training. Training a model by feeding it data in a stream-like fashion is often referred to as online learning. Not all models allow for computationally efficient online learning, but K-Nearest Neighbours does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def update(self, X, y):\n",
    "\n",
    "        self.X_ = np.concatenate((self.X_, X))\n",
    "        self.y_ = np.concatenate((self.y_, y))\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make predictions, we also need to create a predict function. For a K-Nearest Neighbours model, a prediction is made in two steps:\n",
    "\n",
    "- Find the K-nearest neighbours by computing their distances to the data point we want to predict\n",
    "- Given these neighbours and their distances, compute the predicted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict(self, X):\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for x in X:\n",
    "\n",
    "            neighbours, distances = self._get_neighbours(x)\n",
    "\n",
    "            prediction = self._vote(neighbours, distances)\n",
    "\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving the neighbours can be done by calculating all pairwise distances between the data point and the data stored inside the state of the model. Once these distances are known, the K instances that have the shortest distance to the example are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _get_neighbours(self, x):\n",
    "\n",
    "        distances = np.array([self._distance(x, x_) for x_ in self.X_])\n",
    "        indices = np.argsort(distances)[:self.k]\n",
    "\n",
    "        return self.y_[indices], distances[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we would like to use weighted distances, we need to compute the weights. By default, these weights are all set to 1 to make all instances equal. To weigh the instances, neighbours that are closer are typically favoured by given them a weight equal to 1 divided by their distance.\n",
    "\n",
    ">If neighbours have distance 0, since we can’t divide by zero, their weight is set to 1, and all other weights are set to 0. This is also how scikit-learn deals with this problem according to their source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _get_weights(self, distances):\n",
    "\n",
    "        weights = np.ones_like(distances, dtype=float)\n",
    "\n",
    "        if self.weighted:\n",
    "            if any(distances == 0):\n",
    "                weights[distances != 0] = 0\n",
    "            else:\n",
    "                weights /= distances\n",
    "\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only function that we have yet to define is the vote function that is called in the predict function. Depending on the implementation of that function, K-Nearest Neighbours can be used for regression, classification, or even as a meta-learner. \n",
    "\n",
    "## KNN for Regression\n",
    "\n",
    "In order to use K-Nearest Neighbour for regression, the vote function is defined as the average of the neighbours. In case weighting is used, the vote function returns the weighted average, favouring closer instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_Regressor(KNN):\n",
    " \n",
    "    def _vote(self, targets, distances):\n",
    " \n",
    "        weights = self._get_weights(distances)\n",
    " \n",
    "        return np.sum(weights * targets) / np.sum(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN for Classification\n",
    "\n",
    "In the classification case, the vote function uses a majority voting scheme. If weighting is used, each neighbour has a different impact on the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_Classifier(KNN):\n",
    " \n",
    "    def _vote(self, classes, distances):\n",
    " \n",
    "        weights = self._get_weights(distances)\n",
    " \n",
    "        prediction = None\n",
    "        max_weighted_frequency = 0\n",
    " \n",
    "        for c in classes:\n",
    " \n",
    "            weighted_frequency = np.sum(weights[classes == c])\n",
    " \n",
    "            if weighted_frequency > max_weighted_frequency:\n",
    " \n",
    "                prediction = c\n",
    "                max_weighted_frequency = weighted_frequency\n",
    " \n",
    "        return prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
