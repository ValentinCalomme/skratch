{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial NaÃ¯ve Bayes\n",
    "\n",
    "In certain situations, using Bernoulli is not enough. Indeed, sometimes, knowing how many times a feature appears, as opposed to whether it appears at all is necessary.\n",
    "\n",
    ">It is important to note that in some cases, even though it might seem non-intuitive, Bernoulli outperforms Multinomial. It is likely due to the fact that a Multinomial distribution is only affects by the features that do appear. Whereas Bernoulli also takes the absence of features into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial as fact\n",
    "from collections import Counter\n",
    " \n",
    "import scipy.stats as ss\n",
    "import numpy as np\n",
    " \n",
    "from supervised.nb_classifier import NBClassifier\n",
    " \n",
    " \n",
    "class MultinomialNB(NBClassifier):\n",
    " \n",
    "    def __init__(self, alpha=1.0):\n",
    " \n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    " \n",
    "    def _pdf(self, x, p):\n",
    " \n",
    "        f = fact(np.sum(x))\n",
    " \n",
    "        for P, X in zip(p, x):\n",
    "            f *= (P**X) / fact(X)\n",
    " \n",
    "        return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting\n",
    "\n",
    "A Multinomial distribution is parameterized by a single parameter, which depends on the number of times each feature occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _fit_evidence(self, X):\n",
    "\n",
    "        evidence_ = np.sum(X, axis=0)\n",
    "\n",
    "        return evidence_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the likelihood then becomes trivial, as it is similar to fitting the evidence for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _fit_likelihood(self, X, y):\n",
    "\n",
    "        likelihood_ = []\n",
    "\n",
    "        for c in self.classes_:\n",
    "\n",
    "            samples = X[y == c]  # only keep samples of class c\n",
    "\n",
    "            likelihood_.append(self._fit_evidence(samples))\n",
    "\n",
    "        return likelihood_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting\n",
    "\n",
    "Assuming that our model is trained, we need to be able to make use of its state in order to compute the evidence and likelihood. We can then reuse the _pdf that was defined at the beginning.\n",
    "\n",
    "The alpha parameter of the model is there to parameterize the [additive smoothing](https://en.wikipedia.org/wiki/Additive_smoothing). Additive, or Laplace, smoothing helps maintaining non-zero probabilities. No smoothing means keeping the original probability estimate, and maximum smoothing means assuming all probabilities are uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _get_evidence(self, sample):\n",
    "\n",
    "        p = []\n",
    "\n",
    "        for i, feature in enumerate(sample):\n",
    "\n",
    "            x = self.evidence_[i]\n",
    "            N = np.sum(self.evidence_)\n",
    "            d = len(sample)\n",
    "            a = self.alpha\n",
    "\n",
    "            prob = (x + a) / (N + (a * d))\n",
    "\n",
    "            p.append(prob)\n",
    "\n",
    "        return self._pdf(sample, p)\n",
    "\n",
    "    def _get_likelihood(self, sample, c):\n",
    "\n",
    "        p = []\n",
    "\n",
    "        for i, feature in enumerate(sample):\n",
    "\n",
    "            x = self.likelihood_[i]\n",
    "            N = np.sum(self.likelihood_)\n",
    "            d = len(sample)\n",
    "            a = self.alpha\n",
    "\n",
    "            prob = (x + a) / (N + (a * d))\n",
    "\n",
    "            p.append(prob)\n",
    "\n",
    "        return self._pdf(sample, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating\n",
    "\n",
    "Updating the model means that given new data, the counts of features have to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _update_evidence(self, X):\n",
    "\n",
    "        self.evidence_ += np.sum(X, axis=0)\n",
    "\n",
    "        return self.evidence_\n",
    "\n",
    "    def _update_likelihood(self, X, y):\n",
    "\n",
    "        for i, c in enumerate(self.classes_):\n",
    "            samples = X[y == c]   # only keep samples of class c\n",
    "\n",
    "            self.likelihood_[i] += np.sum(samples, axis=0)\n",
    "\n",
    "        return likelihood_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
