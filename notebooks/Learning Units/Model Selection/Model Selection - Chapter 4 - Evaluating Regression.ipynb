{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Regression\n",
    "\n",
    "Machine learning is all about getting better and better at a task. Therefore, we need to define what it means to be good. In the case of regression, it is better to be as close to the target variable. But how do we define close? There exist several performance metrics to evaluate regression.\n",
    "\n",
    "## Mean-Squared Error\n",
    "\n",
    "The mean-squared error is probably the most commonly used metric for regression. It is often set as the default metric in many machine learning packages. It is defined as the average of the square of the errors. It loosely means that large errors are proportionally worse than small mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predicted_target, target):\n",
    "    errors = predicted_target - target\n",
    "     \n",
    "    return np.mean(errors**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean-Squared Error\n",
    "\n",
    "The root mean-squared error is related to the mean squared error. It is simply the square root of the former metric. It has the advantage of being of the same units as the target variable. Therefore, it can be easily interpreted as the average distance of the output to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(predicted_target, target):\n",
    "    return np.sqrt(MSE(predicted_target, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error\n",
    "\n",
    "As opposed to the mean-squared error, the mean absolute error views all errors as proportionally as bad as each other and, therefore, large errors are not penalized more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(output, target):\n",
    "    errors = output - target\n",
    "     \n",
    "    return np.mean(np.abs(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $R^2$\n",
    "\n",
    "$R^2$ is also often referred to as the coefficient of determination, or the explained variance. It represents how much of the targetâ€™s variance can be explained by the data. 1 is best, lower is worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RSquared(predicted_target, target):\n",
    "    numerator = np.sum((target - predicted_target)**2)\n",
    "    denominator = np.sum((target - np.mean(target))**2)\n",
    "     \n",
    "    return 1.0 - (numerator / denominator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Metrics\n",
    "\n",
    "A simple example would be to use weighted versions of the aforementioned metrics. By doing this, you would loosely make it more important to perform well for certain data points than others. It could also be possible to have a fully custom metric based on a custom error function. Perhaps, your application entails that it is much worse to overshoot rather than undershoot for instance.\n",
    "\n",
    "The metric should ultimately represent what it means for your regression to be good, whatever it may mean in your application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
