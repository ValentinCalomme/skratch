{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cd6a64cf2b8a4c0f",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is a supervised task where a model maps input to a discrete output, often referred to as the class. Classification is a specific case of regression. More formally, a classification problem can be defined as learning a function $f$ that will map input variables $X = x_0, x_1,\\dots, x_{m-1}, x_{m}$ to a discrete target variable $y$ such that $f(x) = y$.\n",
    "\n",
    "So for instance, let's say that we have the following data:\n",
    "\n",
    "| Variable 1 | Variable 1 | Variable 3 | Variable 4 | Target variable |\n",
    "|------------|------------|------------|------------|:---------------:|\n",
    "| 1          | 2          | 3          | 4          | Yes              |\n",
    "| 2          | 3          | 4          | 5          | No              |\n",
    "| 3          | 4          | 5          | 6          | Yes              |\n",
    "| ...        | ...        | ...        | ...        | ...             |\n",
    "| 2000       | 2001       | 2002       | 2003       | No            |\n",
    "\n",
    "We would want to learn some function such that $f(1,2,3,4) = Yes$ and $f(2,3,4,5) = No$ and so on.\n",
    "\n",
    "Classification is often seen as finding decision boundaries in data. Finding a boundary which separates the multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Title](../../../source/visualization/images/classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating classification\n",
    "\n",
    "Machine learning is all about getting better and better at a task. Therefore, we need to define what it means to be _good_.\n",
    "\n",
    "For instance, given the output of different models compared to the target variable, which model would you say is better, and why?\n",
    "\n",
    "| Target |0|0|0|3|1|0|1|1|0|1|1|0|3|3|0|2|1|1|3|3|\n",
    "|:-----:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|Model A|3|0|3|2|1|3|2|2|0|1|1|3|1|0|0|0|0|1|0|3|\n",
    "|Model B|3|3|0|1|0|1|2|0|1|0|1|0|1|3|1|3|3|1|0|0|\n",
    "|Model C|3|0|1|2|0|0|1|1|3|1|3|3|3|0|1|2|0|1|0|0|\n",
    "|Model D|0|3|0|1|3|1|1|0|1|3|1|3|0|0|3|0|2|1|1|0|\n",
    "|Model E|3|1|3|0|0|0|2|1|1|0|1|3|0|1|1|0|0|3|1|3|\n",
    "\n",
    "This might be difficult to tell, especially if there are more models and predictions. Thankfully, when it comes to classification, there exist several commonly-used metrics to tackle this problem. Let's use the data from the table as example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "target = np.array([0, 0, 0, 3, 1, 0, 1, 1, 0, 1, 1, 0, 3, 3, 0, 2, 1, 1, 3, 3])\n",
    "\n",
    "predictions = {'A': np.array([3, 0, 3, 2, 1, 3, 2, 2, 0, 1, 1, 3, 1, 0, 0, 0, 0, 1, 0, 3]),\n",
    "               'B': np.array([3, 3, 0, 1, 0, 1, 2, 0, 1, 0, 1, 0, 1, 3, 1, 3, 3, 1, 0, 0]),\n",
    "               'C': np.array([3, 0, 1, 2, 0, 0, 1, 1, 3, 1, 3, 3, 3, 0, 1, 2, 0, 1, 0, 0]),\n",
    "               'D': np.array([0, 3, 0, 1, 3, 1, 1, 0, 1, 3, 1, 3, 0, 0, 3, 0, 2, 1, 1, 0]),\n",
    "               'E': np.array([3, 1, 3, 0, 0, 0, 2, 1, 1, 0, 1, 3, 0, 1, 1, 0, 0, 3, 1, 3])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "$Accuracy = \\frac{\\textrm{tp} + \\textrm{tn}}{\\textrm{tp} + \\textrm{tn} + \\textrm{fp} + \\textrm{fn}}$\n",
    "\n",
    "The accuracy is a straight-forward method which tells us how many classes were correctly predicted overall. It gives a clear idea of how well a model is performing when classes are balanced. Indeed, if classes aren't balanced, let's say 95% class 1 and 5% class 2, we could always predict class 1 and already have an accuracy of 95%. But we might not want to never predict class 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 0.4000\n",
      "B: 0.2500\n",
      "C: 0.4000\n",
      "D: 0.2500\n",
      "E: 0.2000\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predicted_target, target):\n",
    "    \n",
    "    return np.mean(target == predicted_target)\n",
    "\n",
    "for model_name, predicted_target in predictions.items():\n",
    "    print(f\"{model_name}: {accuracy(predicted_target, target):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "To remedy the flaws of simple accuracy, there exist metrics like recall. The recall for each class is defined as the number of instance of that class that were correctly predicted (true positives), divided by the total number of instances of that class (true positives and false negatives).\n",
    "\n",
    "In other words, if a classifier only predicts class 1 and never class 2, it will have a recall of 1 for class 1 but a recall of 0 for class 2.\n",
    "\n",
    "Recall is a metric that is calculated for each class. In order to provide a single metric for the classification problem as a whole, we typically compute a weighted average based on how many times each class appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: weighted recall(0.40)| class 0 (0.43)| class 1 (0.57)| class 2 (0.00)| class 3 (0.20)\n",
      "B: weighted recall(0.25)| class 0 (0.29)| class 1 (0.29)| class 2 (0.00)| class 3 (0.20)\n",
      "C: weighted recall(0.40)| class 0 (0.29)| class 1 (0.57)| class 2 (1.00)| class 3 (0.20)\n",
      "D: weighted recall(0.25)| class 0 (0.29)| class 1 (0.43)| class 2 (0.00)| class 3 (0.00)\n",
      "E: weighted recall(0.20)| class 0 (0.14)| class 1 (0.29)| class 2 (0.00)| class 3 (0.20)\n"
     ]
    }
   ],
   "source": [
    "def recall(predicted_target, target):\n",
    "\n",
    "    recall_per_class = {}\n",
    "    classes, counts = np.unique(target, return_counts=True)\n",
    "\n",
    "    for c in classes:\n",
    "        recall_per_class[c] = np.sum((target == c) & (predicted_target == c)) / np.sum(target == c)\n",
    "\n",
    "    weighted_recall = np.sum([recall_per_class[c] * counts[c] / len(target) for c in classes])\n",
    "\n",
    "    return weighted_recall, recall_per_class\n",
    "\n",
    "for model_name, predicted_target in predictions.items():\n",
    "    weighted_recall, recall_per_class = recall(predicted_target, target)\n",
    "    \n",
    "    print(f\"{model_name}: weighted recall({weighted_recall:.2f})\", end=\"\")\n",
    "    print(f\"| class 0 ({recall_per_class[0]:.2f})\", end=\"\")\n",
    "    print(f\"| class 1 ({recall_per_class[1]:.2f})\", end=\"\")\n",
    "    print(f\"| class 2 ({recall_per_class[2]:.2f})\", end=\"\")\n",
    "    print(f\"| class 3 ({recall_per_class[3]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "$Precision = \\frac{tp}{\\textrm{tp} + \\textrm{fp}}$\n",
    "\n",
    "Another metric that remedies the flaws of simple accuracy is precision. The precision represents how accurate a model is at predicting each class. In other words, if a classifier only recalls 1 instance of class 1 but predicts it correctly, it will have a precision of 1, whereas if it predicts half of class 1 correctly, its precision will be 0.5.\n",
    "\n",
    "Precision is a metric that is calculated for each class in a classification problem. In order to provide a single metric, we typically compute the weighted average of the precision of each class, based on how many times each class appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: weighted precision(0.48)| class 0 (0.43)| class 1 (0.80)| class 2 (0.00)| class 3 (0.20)\n",
      "B: weighted precision(0.25)| class 0 (0.29)| class 1 (0.29)| class 2 (0.00)| class 3 (0.20)\n",
      "C: weighted precision(0.41)| class 0 (0.29)| class 1 (0.67)| class 2 (0.50)| class 3 (0.20)\n",
      "D: weighted precision(0.25)| class 0 (0.29)| class 1 (0.43)| class 2 (0.00)| class 3 (0.00)\n",
      "E: weighted precision(0.20)| class 0 (0.14)| class 1 (0.29)| class 2 (0.00)| class 3 (0.20)\n"
     ]
    }
   ],
   "source": [
    "def precision(predicted_target, target):\n",
    "\n",
    "    precision_per_class = {}\n",
    "    classes, counts = np.unique(target, return_counts=True)\n",
    "\n",
    "    for c in classes:\n",
    "        precision_per_class[c] = np.sum((target == c) & (predicted_target == c)) / np.sum(predicted_target == c)\n",
    "\n",
    "    weighted_precision = np.sum([precision_per_class[c] * counts[c] / len(target) for c in classes])\n",
    "\n",
    "    return weighted_precision, precision_per_class\n",
    "\n",
    "for model_name, predicted_target in predictions.items():\n",
    "    weighted_precision, precision_per_class = precision(predicted_target, target)\n",
    "    \n",
    "    print(f\"{model_name}: weighted precision({weighted_precision:.2f})\", end=\"\")\n",
    "    print(f\"| class 0 ({precision_per_class[0]:.2f})\", end=\"\")\n",
    "    print(f\"| class 1 ({precision_per_class[1]:.2f})\", end=\"\")\n",
    "    print(f\"| class 2 ({precision_per_class[2]:.2f})\", end=\"\")\n",
    "    print(f\"| class 3 ({precision_per_class[3]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $F_\\beta$ Score\n",
    "\n",
    "$F_{\\beta} = (1 + \\beta^2) \\cdot \\frac{\\textrm{precision} \\cdot \\textrm{recall}}{(\\beta^2 \\cdot \\textrm{precision}) + \\textrm{recall}}$\n",
    "\n",
    "A good model should have a high recall and a high precision. In some cases, one matters more than the other. The $F_\\beta$ measure is defined as the weighted harmonic mean of the recall and the precision and therefore provides a single metric combining the two. The $\\beta$ parameter influences the weight given to the precision. If it is set to 1, precision and recall are weighted equally. If it is less than 1, recall is favoured. If it is more than 1, precision is favoured. Typically, $\\beta$ is set to 1 and the measure is then called the $F_1$ score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: weighted fbeta_score(0.43)| class 0 (0.43)| class 1 (0.67)| class 2 (0.00)| class 3 (0.20)\n",
      "B: weighted fbeta_score(0.25)| class 0 (0.29)| class 1 (0.29)| class 2 (0.00)| class 3 (0.20)\n",
      "C: weighted fbeta_score(0.40)| class 0 (0.29)| class 1 (0.62)| class 2 (0.67)| class 3 (0.20)\n",
      "D: weighted fbeta_score(0.25)| class 0 (0.29)| class 1 (0.43)| class 2 (0.00)| class 3 (0.00)\n",
      "E: weighted fbeta_score(0.20)| class 0 (0.14)| class 1 (0.29)| class 2 (0.00)| class 3 (0.20)\n"
     ]
    }
   ],
   "source": [
    "def fbeta_score(predicted_target, target, beta=1.0):\n",
    "\n",
    "    classes, counts = np.unique(target, return_counts=True)\n",
    "    fbeta_score_per_class = {c:0 for c in classes}\n",
    "\n",
    "    # Computes the recall and precision of these classes\n",
    "    _, p = precision(predicted_target, target)\n",
    "    _, r = recall(predicted_target, target)\n",
    "\n",
    "    # Computes the F-beta score as the harmonic mean between precision and recall\n",
    "    for c in classes:\n",
    "        if beta**2 * p[c] + r[c] == 0:\n",
    "            fbeta_score_per_class[c] = 0 # if precision and recall are 0, then f-beta should also be zero\n",
    "        else:\n",
    "            fbeta_score_per_class[c] = (1 + beta**2) * (p[c] * r[c]) / (beta**2 * p[c] + r[c])\n",
    "\n",
    "    weighted_fbeta_score = np.sum([fbeta_score_per_class[c] * counts[c] / len(target) for c in classes])\n",
    "\n",
    "    return weighted_fbeta_score, fbeta_score_per_class\n",
    "\n",
    "for model_name, predicted_target in predictions.items():\n",
    "    weighted_fbeta_score, fbeta_score_per_class = fbeta_score(predicted_target, target)\n",
    "    \n",
    "    print(f\"{model_name}: weighted fbeta_score({weighted_fbeta_score:.2f})\", end=\"\")\n",
    "    print(f\"| class 0 ({fbeta_score_per_class[0]:.2f})\", end=\"\")\n",
    "    print(f\"| class 1 ({fbeta_score_per_class[1]:.2f})\", end=\"\")\n",
    "    print(f\"| class 2 ({fbeta_score_per_class[2]:.2f})\", end=\"\")\n",
    "    print(f\"| class 3 ({fbeta_score_per_class[3]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the difference between precision and recall\n",
    "\n",
    "\n",
    "Precision and recall both express related but conflicting concepts. To really understand the difference between the two, let's use an example. Imagine that you are an admission officer at a university. Your goal is to classify the applications you receive such that you admit all of the good candidates, and none of the bad ones.\n",
    "\n",
    "If you decide to focus only on recall, it means that your goal is to admit as many good candidates as possible, regardless of how many bad candidates you admit by mistake. If you decide to focus only on precision, it means that your goal is that every single candidate you admit is a good candidate, regardless of whether you rejected some good candidates by mistake.\n",
    "\n",
    "Accepting all applications will increase the recall, but it will reduce the precision. Similarly, being extremely strict about which application to accept will increase the precision, but will reduce the recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom metrics\n",
    "\n",
    "Of course, it is completely possible to use custom metrics.\n",
    "\n",
    "A simple example would be to use weighted versions of the aforementioned metrics. By doing this, you would loosely make it more important to perform well for certain classes than others. It could also be possible to have a fully custom metric based on a custom error function. Perhaps, your application entails that it is much worse to confuse class 1 with class 2 than it is to confuse it with class 3 for instance.\n",
    "\n",
    "The metric should ultimately represent what it means for your classification to be good, whatever it may mean in your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Examples\n",
    "\n",
    "### Detecting spam e-mails\n",
    "\n",
    "_Input variables_: Number of times certain words appear in the e-mail\n",
    "_Target variable_: Whether an e-mail is a spam\n",
    "\n",
    "| Spam | Mom | Loan | Hello | Spam |\n",
    "|:----:|:---:|:----:|:-----:|:----:|\n",
    "|   4  |  0  |   3  |   0   |  Yes |\n",
    "|   0  |  1  |   0  |   1   |  No  |\n",
    "|   0  |  3  |   1  |   2   |  No  |\n",
    "|  ... | ... |  ... |  ...  |   1  |\n",
    "| 5    | 0   | 0    | 1     | Yes  |\n",
    "\n",
    "This could be useful to create a spam filter.\n",
    "\n",
    "### Facial recognition\n",
    "\n",
    "_Input variables_: Intensity of pixels in a 100x100 photo\n",
    "_Target variable_: Person\n",
    "\n",
    "| (0,0) | (0,1) | ... | (99,98) | (99,99) | Person   |\n",
    "|:-----:|:-----:|:---:|:-------:|:-------:|----------|\n",
    "|  0.81 |  0.72 | ... |   0.41  |   0.55  | Valentin |\n",
    "|  0.23 |  0.12 | ... |   0.07  |   0.92  | Jack     |\n",
    "|  0.54 |  0.48 | ... |    0    |   0.31  | Robin    |\n",
    "|  ...  |  ...  | ... |   ...   |   ...   | ...      |\n",
    "| 0.71  | 0.79  | ... | 0.37    | 0.81    | Lisa     |\n",
    "\n",
    "This could be used by an application that tags pictures automatically.\n",
    "\n",
    "### Predict whether a team will win a basketball game\n",
    "\n",
    "_Input variables_: Win percentage, win percentage of the opponent, rebound per game, rebound per game by the opponent\n",
    "_Target variable_: Likelihood of defaulting\n",
    "\n",
    "| Win % | Opponent Win % |  RPG | Opponent RPG | Will win the game |\n",
    "|:-----:|:--------------:|:----:|:------------:|:-----------------:|\n",
    "|  0.65 |      0.33      | 45.8 |     42.2     |        Yes        |\n",
    "|  0.54 |      0.47      | 37.6 |     44.3     |         No        |\n",
    "|  0.28 |      0.77      | 38.1 |     48.7     |         No        |\n",
    "|  ...  |       ...      |  ... |      ...     |        ...        |\n",
    "| 0.38  | 0.43           | 37.8 | 36.9         | Yes               |\n",
    "\n",
    "This could be used by a team to know when they could rest their star players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
