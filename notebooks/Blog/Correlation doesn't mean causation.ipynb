{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misinterpreting results\n",
    "\n",
    "Correlation doesn’t mean causation.\n",
    "\n",
    "In machine learning, we are focused on performing a task well. Which means that in regression, we are more interested in predicting the correct output, than we are in understanding how the input affects the output.\n",
    "\n",
    "Perhaps, in some cases, some variables are more important than others, and this is perfectly fine. It’s important to make a distinction between “these variables are important to help me make a good prediction” and “these variables are causing this to happen”.\n",
    "\n",
    "Some models that will be covered later on will be said to be white-box and others will be said to be black-box. Indeed, some models offer clearer insights on how they interpret the variables, but this still doesn’t mean that they somehow offer more clarity on the causal effect of variables. Proving causality would require more than machine learning.\n",
    "\n",
    "Perhaps X doesn’t affect Y but is a “victim” of a third variable Z → Example of Freakonomics with books and smart kids (correlation but perhaps no causation) → such variables are called latent variables and are an area of focus in machine learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
